{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "institutional-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python code used to produce the results and figures for the paper:\n",
    "## NCYPred: A Bidirectional Long Short-term Memory Network with Attention Mechanism \n",
    "##         for Y RNA and small non-coding RNA classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-yahoo",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from Bio import SeqIO\n",
    "\n",
    "sys.path.insert(1, './python-functions/')\n",
    "\n",
    "from my_functions import CreateDf, SeqTo3mer, Remover, TokenPad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-radio",
   "metadata": {},
   "source": [
    "## Data processing and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "val_set = './dataset/validation-set-rfam.csv'\n",
    "df_val = pd.read_csv(val_set, index_col=0)\n",
    "\n",
    "# label one-hot encoding\n",
    "one_hot_labels = pd.get_dummies(df_val['labels']).values\n",
    "df_val['b-labels'] = list(one_hot_labels)\n",
    "\n",
    "# formatting\n",
    "array_list = []\n",
    "for arr in list(df_val['b-labels']):\n",
    "    array_list.append(arr)\n",
    "    \n",
    "y_b = np.vstack(array_list) # label array\n",
    "\n",
    "# decompose sequences into 3-mers\n",
    "X = df_val['seq']\n",
    "X = SeqTo3mer(X)\n",
    "\n",
    "# load tokenizer\n",
    "with open('./tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "# tokenize and zero padding\n",
    "X_pad = TokenPad(X, 498, 'post', tokenizer)\n",
    "\n",
    "# load model\n",
    "biLSTMAtt = keras.models.load_model('./trained-model/')\n",
    "\n",
    "# prediction\n",
    "predictions = biLSTMAtt.predict(X_pad)\n",
    "argmax_bilstmatt = np.argmax(predictions, axis=1)\n",
    "\n",
    "# argmax true labels\n",
    "argmax_valid = np.argmax(y_b, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-strain",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-broad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (classification_report,\n",
    "\t\t\t\t\t\t\t confusion_matrix,\n",
    "\t\t\t\t\t\t\t roc_auc_score)\n",
    "\n",
    "label_list = ['5.8S-rRNA', '5S-rRNA', 'CD-box', 'HACA-box', 'Intron-gp-I', \n",
    "              'Intron-gp-II', 'Leader', 'Riboswitch', 'Ribozyme', 'Y-RNA',\n",
    "              'Y-RNA-like', 'miRNA ', 'tRNA']\n",
    "\n",
    "\n",
    "def plot_cm(labels, argmax_prediction, normalize, fmt, title):\n",
    "    cm = confusion_matrix(labels, argmax_prediction, normalize=normalize)\n",
    "    fig = plt.figure(figsize=(10, 8), dpi=350)\n",
    "    sns.set(font_scale=1.15)\n",
    "    sns.heatmap(cm, cmap='Reds', fmt=fmt, square=True, annot=True)\n",
    "    plt.title(title, fontsize=22)\n",
    "    plt.ylabel(\"True label\", fontsize=20)\n",
    "    plt.xlabel(\"Predicted label\", fontsize=20)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xticklabels(label_list)\n",
    "    ax.set_yticklabels(label_list)\n",
    "    plt.yticks(rotation='horizontal', fontsize=18)\n",
    "    plt.xticks(rotation='vertical', fontsize=18)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(argmax_valid, argmax_bilstmatt)\n",
    "print(report)\n",
    "title = 'Validation set'\n",
    "plot_cm(argmax_valid, argmax_bilstmatt, 'true','.2f', title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-wonder",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(argmax_valid, argmax_bilstmatt)\n",
    "\n",
    "FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
    "FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "TP = np.diag(cnf_matrix)\n",
    "TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "print(TPR, 'recall')\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "print(TNR, 'specificity')\n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "print(PPV, 'precision')\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy for each class\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "print(ACC, 'accuracy')\n",
    "# F1-score\n",
    "F1 = (2*TP)/((2*TP)+FP+FN)\n",
    "print(F1, 'f1')\n",
    "# Matthews correlation coefficient (MCC)\n",
    "MCC = ((TP*TN)-(FP*FN))/np.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))\n",
    "print(MCC, 'MCC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall accuracy\n",
    "print((sum(TP)+sum(TN))/(sum(TP)+sum(TN)+sum(FP)+sum(FN)), 'accuracy')\n",
    "\n",
    "# overall sensitivity\n",
    "print(sum(TP)/(sum(TP)+sum(FN)), 'sensitivity')\n",
    "\n",
    "# overall specificity\n",
    "print(sum(TN)/(sum(TN)+sum(FP)), 'specificity')\n",
    "\n",
    "# overall precision\n",
    "print(sum(TP)/(sum(TP)+sum(FP)), 'precision')\n",
    "\n",
    "# overall f1-score\n",
    "print((2*(sum(TP)))/((2*(sum(TP)))+sum(FP)+sum(FN)), 'f1-score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-hughes",
   "metadata": {},
   "source": [
    "# ROC and Precision-recall curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = y_b\n",
    "y_score = pred_bilstmatt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# references\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(13):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i],\n",
    "                                                        y_score[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test[:, i], y_score[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test.ravel(),\n",
    "    y_score.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test, y_score,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "      .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.step(recall['micro'], precision['micro'], where='post')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(\n",
    "    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n",
    "    .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-recall multiclass\n",
    "\n",
    "colors = ['r', 'g', 'b', 'c', 'sienna', 'y', 'grey', 'darkorange', 'lime', \n",
    "                'purple', 'cyan', 'palegreen', 'magenta']\n",
    "\n",
    "classes = ['5.8S-rRNA', '5S-rRNA', 'CD-box', 'HACA-box', 'Intron-gp-I', 'Intron-gp-II', 'Leader',\n",
    "           'Riboswitch', 'Ribozyme', 'Y-RNA', 'Y-RNA-like', 'miRNA', 'tRNA']\n",
    "\n",
    "\n",
    "n_classes = 13\n",
    "\n",
    "plt.figure(figsize=(4,4), dpi=350)\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "pr_auc = []\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('{0} ({1:0.3f})'\n",
    "                  ''.format(classes[i], average_precision[i]))\n",
    "    pr_auc.append(average_precision[i])\n",
    "    \n",
    "plt.plot(recall['micro'], precision['micro'], color='black', linestyle='dashed', lw=1.5)\n",
    "plt.xlim([-0.015, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall', fontsize=20)\n",
    "plt.ylabel('Precision', fontsize=20)\n",
    "#plt.title('Precision-Recall curve')\n",
    "#plt.legend(lines, labels, loc=(0, 1.5), prop=dict(size=14), ncol=2)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-suspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(4,4), dpi=350)\n",
    "\n",
    "colors = cycle(['r', 'g', 'b', 'c', 'sienna', 'y', 'grey', 'darkorange', 'lime', \n",
    "                'purple', 'cyan', 'palegreen', 'magenta'])\n",
    "\n",
    "label_list = ['5.8S-rRNA', '5S-rRNA', 'CD-box', 'HACA-box', 'Intron-gp-I', 'Intron-gp-II', 'Leader',\n",
    "           'Riboswitch', 'Ribozyme', 'Y-RNA', 'Y-RNA-like', 'miRNA', 'tRNA']\n",
    "\n",
    "\n",
    "roc_auc_list = []\n",
    "\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color,\n",
    "             label='{0} '\n",
    "             ''.format(label_list[i]))\n",
    "    roc_auc_list.append(roc_auc[i])\n",
    "\n",
    "\n",
    "plt.plot(fpr['micro'], tpr['micro'], color='black', linestyle='dashed', lw=1, label='micro-average')\n",
    "plt.plot([0, 1], [0, 1], 'k-',lw=1)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=20)\n",
    "plt.ylabel('True Positive Rate', fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-adult",
   "metadata": {},
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X):\n",
    "    \n",
    "    \"\"\" Function used to extract the context vector and attention weights while predicting samples \"\"\"\n",
    "    \n",
    "    # input: model, data, attention layer index, output distrbution from attention layer\n",
    "    \n",
    "    predictions = tf.keras.Model(inputs = model.input, \n",
    "                                 outputs = [model.output,\n",
    "                                            model.layers[3].output[0],\n",
    "                                            model.layers[3].output[1]])\n",
    "    \n",
    "    output, context, weights = predictions.predict(X)\n",
    "    \n",
    "    # return as dataframe\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['prediction'] = np.argmax(output, axis=1)\n",
    "    new_df['c_vector'] = list(context)\n",
    "    new_df['weights'] = weights\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne(df, p):\n",
    "    \n",
    "    \" Used for plotting t-SNE results \"\n",
    "\n",
    "    colors = ['r', 'g', 'b', 'c', 'sienna', 'y', 'grey', 'darkorange', 'lime', 'purple', 'cyan', 'palegreen', 'magenta']\n",
    "\n",
    "    classes = ['5.8S-rRNA', '5S-rRNA', 'CD-box', 'HACA-box', 'Intron-gp-I', 'Intron-gp-II', 'Leader',\n",
    "               'Riboswitch', 'Ribozyme', 'Y-RNA', 'Y-RNA-like', 'miRNA', 'tRNA']\n",
    "    \n",
    "    plt.figure(figsize=(7, 6), dpi=350)\n",
    "    \n",
    "    for target, color in zip(classes, colors):\n",
    "        indicesToKeep = output['true'] == target\n",
    "    \n",
    "        plt.scatter(df.loc[indicesToKeep, 'X']\n",
    "                   , df.loc[indicesToKeep, 'Y']\n",
    "                   , c = color\n",
    "                   , s = 50\n",
    "                   , alpha=0.9\n",
    "                   , edgecolors='black'\n",
    "                   , linewidths=0.6)\n",
    "        \n",
    "    plt.title('Perplexity: {}'.format(p), fontsize=14)\n",
    "    plt.xlabel('X', fontsize=14)\n",
    "    plt.ylabel('Y', fontsize=14)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    plt.tick_params(axis='both', which='minor', labelsize=12)\n",
    "    #plt.legend(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predict(biLSTMAtt, X_pad)\n",
    "output['true'] = list(df_val['labels'])\n",
    "output[list(range(64))] = pd.DataFrame(output.c_vector.tolist(), index= output.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-makeup",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = [10, 25, 50]\n",
    "\n",
    "for p in perplexities:\n",
    "\n",
    "    X_embedded = TSNE(n_components=2, perplexity=p, init='pca', n_jobs=3).fit_transform(c)\n",
    "    df_tsne = pd.DataFrame(data = X_embedded,\n",
    "                           columns = ['X', 'Y'])\n",
    "    df_tsne['prediction'] = output['prediction']\n",
    "\n",
    "    plot_tsne(df_tsne, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-tragedy",
   "metadata": {},
   "source": [
    "## Attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict samples\n",
    "prediction = predict(biLSTMAtt, X_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store data as a dataframe\n",
    "att_weights = pd.DataFrame()\n",
    "att_weights[list(range(498))] = pd.DataFrame(prediction.weights.tolist(), index= prediction.index)\n",
    "att_weights['prediction'] = prediction['prediction']\n",
    "attention = att_weights.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20), dpi=350)\n",
    "\n",
    "yticks = [1, 24, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325 ]\n",
    "\n",
    "yticklabels = ['', '5.8S-rRNA', '5S-rRNA', 'CD-box', 'HACA-box', 'Intron-gp-I', 'Intron-gp-II',\n",
    "        'Leader', 'Riboswitch', 'Ribozyme', 'Y-RNA', 'Y-RNA-like', 'miRNA', 'tRNA'] \n",
    "\n",
    "ax1 = sns.heatmap(attention, square=True, cbar_kws={\"shrink\": 0.5})\n",
    "ax1.set_yticks(yticks)\n",
    "ax1.set_yticklabels(yticklabels, fontsize=20)\n",
    "ax1.tick_params(axis='y', length=20)\n",
    "plt.xlabel('Token position', fontsize=24)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
