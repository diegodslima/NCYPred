{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NCYPred-training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2DkGYrFZLGw"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import sys\n",
        "from Bio import SeqIO\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "sys.path.insert(1, './python-functions/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-SEVcfeloFK"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yVwyGTMZMkk"
      },
      "source": [
        "from my_functions import SeqTo3mer, Remover, TokenPad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH-9p52XZN1s"
      },
      "source": [
        "# load training set\n",
        "df_train = pd.read_csv('training-set.csv', sep=',', index_col=0)\n",
        "\n",
        "# remove unallowed characters\n",
        "df_train = Remover(df_train)\n",
        "\n",
        "# label one-hot encoding\n",
        "one_hot_labels = pd.get_dummies(df_train['labels']).values\n",
        "df_train['b-labels'] = list(one_hot_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E2ATHZDsJ6R"
      },
      "source": [
        "# label formatting\n",
        "array_list = []\n",
        "for arr in list(df_train['b-labels']):\n",
        "  array_list.append(arr)\n",
        "\n",
        "y_b = np.vstack(array_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWcLKcXaZXAs"
      },
      "source": [
        "# preprocessing\n",
        "X = df_train['seq'].str.upper()\n",
        "y = df_train['labels']\n",
        "\n",
        "# k-mer generation\n",
        "X = SeqTo3mer(X)\n",
        "\n",
        "# tokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_VOCAB_SIZE = 64\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "# tokenizer and zero-padding\n",
        "X_pad = TokenPad(X, 498, 'post', tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZNI-pNmcIP0"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra01TDbxcW7u"
      },
      "source": [
        "from tensorflow.keras.layers import Concatenate, Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import initializers, regularizers, constraints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_EA8CSSDfMx"
      },
      "source": [
        "# Attention layer (supports masking)\n",
        "# Author: Christos Baziotis\n",
        "# https://gist.github.com/cbaziotis/6428df359af27d58078ca5ed9792bd6d\n",
        "\n",
        "def dot_product(x, kernel):\n",
        "    \"\"\"\n",
        "    Wrapper for dot product operation, in order to be compatible with both\n",
        "    Theano and Tensorflow\n",
        "    Args:\n",
        "        x (): input\n",
        "        kernel (): weights\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    if K.backend() == 'tensorflow':\n",
        "        # todo: check that this is correct\n",
        "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(x, kernel)\n",
        "\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True,\n",
        "                 return_attention=False,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        Keras Layer that implements an Attention mechanism for temporal data.\n",
        "        Supports Masking.\n",
        "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
        "        # Input shape\n",
        "            3D tensor with shape: `(samples, steps, features)`.\n",
        "        # Output shape\n",
        "            2D tensor with shape: `(samples, features)`.\n",
        "        :param kwargs:\n",
        "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "        The dimensions are inferred based on the output shape of the RNN.\n",
        "\n",
        "\n",
        "        Note: The layer has been tested with Keras 1.x\n",
        "\n",
        "        Example:\n",
        "        \n",
        "            # 1\n",
        "            model.add(LSTM(64, return_sequences=True))\n",
        "            model.add(Attention())\n",
        "            # next add a Dense layer (for classification/regression) or whatever...\n",
        "\n",
        "            # 2 - Get the attention scores\n",
        "            hidden = LSTM(64, return_sequences=True)(words)\n",
        "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
        "\n",
        "        \"\"\"\n",
        "        self.supports_masking = True\n",
        "        self.return_attention = return_attention\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape=(input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        eij = dot_product(x, self.W)\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        weighted_input = x * K.expand_dims(a)\n",
        "\n",
        "        result = K.sum(weighted_input, axis=1)\n",
        "\n",
        "        if self.return_attention:\n",
        "            return [result, a]\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.return_attention:\n",
        "            return [(input_shape[0], input_shape[-1]),\n",
        "                    (input_shape[0], input_shape[1])]\n",
        "        else:\n",
        "            return input_shape[0], input_shape[-1]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjFrSrwziiuk"
      },
      "source": [
        "def biLSTMAtt(lr):\n",
        "\n",
        "\n",
        "  sequence_input = Input(shape=(498,), dtype=\"int32\")\n",
        "  embedded_sequences = Embedding(64, 10, mask_zero=True)(sequence_input)\n",
        "\n",
        "  lstm = Bidirectional(LSTM(32, input_shape=(498, 64), return_sequences=True)(embedded_sequences)\n",
        "\n",
        "  context_vector, weights = Attention(return_attention=True)(lstm)\n",
        "\n",
        "  dense1 = Dense(128, activation=\"relu\")(context_vector)\n",
        "  dropout = Dropout(0.4)(dense1)\n",
        "  output = Dense(13, activation='softmax')(dropout)\n",
        "\n",
        "  model = tf.keras.Model(inputs=sequence_input, outputs=output)\n",
        "  \n",
        "  # compile model\n",
        "  opt = tf.keras.optimizers.RMSprop(lr=lr, momentum=0.01, centered=True)\n",
        "  \n",
        "  METRICS = [\n",
        "\t  tf.keras.metrics.TruePositives(name='tp'),\n",
        "\t  tf.keras.metrics.FalsePositives(name='fp'),\n",
        "  \ttf.keras.metrics.TrueNegatives(name='tn'),\n",
        "\t  tf.keras.metrics.FalseNegatives(name='fn'),\n",
        "\t  tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "\t  tf.keras.metrics.Precision(name='precision'),\n",
        "\t  tf.keras.metrics.Recall(name='recall'),\n",
        "    ]\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=opt,\n",
        "                metrics=METRICS)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY8JJzofZYUH"
      },
      "source": [
        "## K-fold Cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR1aXUD7i8gI"
      },
      "source": [
        "import pandas\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CGcrCygrn-t"
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "def CreateFolds(X, y, folds, test_size):\n",
        "\n",
        "  sss = StratifiedShuffleSplit(n_splits=folds, test_size=test_size)\n",
        "  sss.get_n_splits(X, y)\n",
        "\n",
        "  count = 1\n",
        "\n",
        "  train_list = []\n",
        "  test_list = []\n",
        "\n",
        "  for train_index, test_index in sss.split(X, y):\n",
        "    print('Fold {}'.format(count))\n",
        "\n",
        "    train_list.append(list(train_index))\n",
        "    test_list.append(list(test_index))\n",
        "\n",
        "    count += 1\n",
        "  \n",
        "  return train_list, test_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-2W9acLlVjR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "388f1a1b-4a98-4b1e-a4c1-dce70ddc3d97"
      },
      "source": [
        "train_list, test_list = CreateFolds(X_pad, y, 10, 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 1\n",
            "Fold 2\n",
            "Fold 3\n",
            "Fold 4\n",
            "Fold 5\n",
            "Fold 6\n",
            "Fold 7\n",
            "Fold 8\n",
            "Fold 9\n",
            "Fold 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05uESml7tqhv"
      },
      "source": [
        "df_train_idx = pd.DataFrame()\n",
        "for i,n in enumerate(train_list):\n",
        "  df_train_idx['fold{}'.format(i+1)] = n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUs7_UWHt3j2"
      },
      "source": [
        "df_test_idx = pd.DataFrame()\n",
        "for i,n in enumerate(test_list):\n",
        "  df_test_idx['fold{}'.format(i+1)] = n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEuh320Z9C91"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqDPtBm8sr5b"
      },
      "source": [
        "description = 'ncRNA - 3-mer'\n",
        "\n",
        "start = 0\n",
        "model_name = 'biLSTMAtt'\n",
        "\n",
        "for i in range(start,10):\n",
        "  print('Fold {}'.format(i+1))\n",
        "  X_train = X_pad[df_train_idx['fold{}'.format(i+1)]]\n",
        "  X_test = X_pad[df_test_idx['fold{}'.format(i+1)]]\n",
        "\n",
        "  y_train = y_b[df_train_idx['fold{}'.format(i+1)]]\n",
        "  y_test = y_b[df_test_idx['fold{}'.format(i+1)]]\n",
        "\n",
        "  print('Train shape: X: {}, y: {}'.format(X_train.shape, y_train.shape))\n",
        "  print('Test shape: X: {}, y: {}'.format(X_test.shape, y_test.shape))\n",
        "  print('Initializing model')\n",
        "\n",
        "  lr = 0.004\n",
        "  epochs = 10\n",
        "  model = biLSTMAtt(lr)\n",
        "\n",
        "  callback = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
        "\n",
        "  print('Training model {} with lr: {}'.format(i+1, lr))\n",
        "  history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), callbacks=[callback])\n",
        "\n",
        "  print('Saving model...')\n",
        "  main_folder = './saved_models/'\n",
        "  current_folder = 'ncypred-SKF-fold{}-lr{}-{}/'.format(i+1, lr, model_name)\n",
        "\n",
        "  model.save(main_folder+current_folder) \n",
        "\n",
        "  print('Saving history at {}'.format(current_folder)) \n",
        "  hist_df = pd.DataFrame(history.history) \n",
        "  hist_csv_file = main_folder+current_folder+'history.csv'\n",
        "\n",
        "  with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)\n",
        "\n",
        "  print('Saving parameters...')\n",
        "  param_file = main_folder+current_folder+'params.dat'\n",
        "  with open(param_file, mode='w') as p:\n",
        "    p.write(description)\n",
        "    p.write('Learning rate: {}'.format(lr))\n",
        "    p.write('Epochs: {}'.format(epochs))\n",
        "\n",
        "  print(' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCNWNmXrN9BZ"
      },
      "source": [
        "# Full training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npff-mT1OBO0"
      },
      "source": [
        "model_name = 'biLSTMAtt'\n",
        "\n",
        "lr = 0.004\n",
        "epochs = 15\n",
        "model = biLSTMAtt(lr)\n",
        "\n",
        "print('Train shape: X: {}, y: {}'.format(X_pad.shape, y_b.shape))\n",
        "\n",
        "history = model.fit(X_pad, y_b, epochs=epochs)\n",
        "\n",
        "print('Saving model...')\n",
        "main_folder = './saved_models/'\n",
        "current_folder = 'ncypred-lr{}-{}/'.format(lr, model_name)\n",
        "\n",
        "model.save(main_folder+current_folder) \n",
        "\n",
        "print('Saving history at {}'.format(current_folder)) \n",
        "hist_df = pd.DataFrame(history.history) \n",
        "hist_csv_file = main_folder+current_folder+'history.csv'\n",
        "\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "  hist_df.to_csv(f)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}